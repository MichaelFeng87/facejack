<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.1.1.min.js"
            integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous">
    </script>
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet"
          href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
          crossorigin="anonymous">
    <!-- Optional theme -->
    <link rel="stylesheet"
          href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css"
          integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp"
          crossorigin="anonymous">
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
            crossorigin="anonymous">
    </script>

    <link href="{{ url_for('static', filename='style.css')}}" rel="stylesheet">

    <title>About faceJack</title>
</head>
<body>

<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#">FaceJack</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
                <li class="stats"><a href="/">Stats</a></li>
                <li class="active"><a href="#">About</a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</nav>

<!-- http://placehold.it/224x224?text=Loading -->

<div class="container">
    <div class="main-container">
	<h1><a id="About_FaceJack_0"></a>About FaceJack</h1>
<h2><a id="Background_2"></a>Background</h2>
<p>Machine learning with deep neural networks (commonly dubbed “deep learning”) has taken the world by storm in the previous years, smashing record after record in a wide variety of difficult tasks (spanning previously largely unrelated fields such as computer vision, speech recognition, natural language processing, etc). One of the computer vision tasks where such gains are clear is the task of <em>facial recognition</em> (identifying a person based on his/her face)—a deep neural network being the primary tool used for this purpose at <a href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/">Facebook</a>, among other places.</p>
<p>One natural extension of the above could be to exploit neural networks within a secure application, in order to authenticate a person based on a shot of their face. Unfortunately, despite the apparently superb performance of such models, it is fairly easy to construct inputs on which the network becomes completely <em>confounded</em> (commonly known as <em>adversarial examples</em>). We built <strong>FaceJack</strong> in order to illuminate this concept. In particular, we’d like to emphasise:</p>
<ul>
<li>how <strong>simple</strong> it is to generate such “fooling” inputs algorithmically, if one has access to the neural network used for facial recognition (either directly or through an API).</li>
<li>how (often <em>imperceptibly</em> to humans) <strong>close</strong> the “fooling” inputs can be to legitimately generated inputs;</li>
<li>how this attack may be executed in <strong>real-time</strong>, requiring only a mid-range GPU.</li>
</ul>
<p>But let’s take it slowly—what even <em>are</em> adversarial inputs?</p>
<h2><a id="Adversarial_training_13"></a>Adversarial training</h2>
<p>Adversarial inputs are made possible by the very <em>design</em> of neural networks. On a high level, a neural network consumes an input, performs several transformations to it, in order to predict a corresponding output. The network parameters are adjusted by running the network on a <em>training set</em> (a set of known input/output pairs from which the network needs to generalise). The network’s transformations are designed to be differentiable, so that the network can be efficiently trained by:</p>
<ul>
<li>Feeding an input to the network, computing a prediction</li>
<li>Computing an <em>error</em> of the prediction with respect to the expected output</li>
<li>Propagating the error backwards through the network, updating parameters as we go.<br>
&lt;image of an MLP here&gt;</li>
</ul>
<p>The network’s differentiability allows us to consider the error function in its parameters, for a fixed input and output—so we can optimise them. However, it also allows considering an error function in the input, for a fixed choice of parameters—so we can modify the <em>input</em> to produce a desirable output. If the “desirable” output classification is one that the original input does not belong to (e.g. classifying my face as John Travolta), then the constructed input represents an <em>adversarial</em> example. Deep neural networks are particularly vulnerable to such inputs, for reasons that are threefold:</p>
<ul>
<li>Computing an adversarial example usually only requires a crude approximation of the gradient of the desired output with respect to the input image—often, only the <em>sign</em> of this gradient for each input pixel is sufficient.</li>
<li>The computed adversarial examples are often <em>imperceptibly similar</em> to the original input—in fact, there is an entire <strong>space</strong> of adversarial inputs surrounding any correctly classified image, as <a href="https://arxiv.org/abs/1312.6199">Szegedy et al.</a> have demonstrated in 2013. &lt;image of an adversarial panda here&gt;</li>
<li>Even worse—what’s adversarial for one network architecture will very often be adversarial for a completely different network as well—as they are often trained on the same datasets!</li>
</ul>
<p>Therefore, deploying neural networks in secure applications requires particular care, as adversarial inputs give rise to a potentially unforeseen <em>covert channel</em> for an exploit.</p>
<h2><a id="What_have_we_done_29"></a>What have we done?</h2>
<p>We have built FaceJack as a simple representative of such an exploit:</p>
<ul>
<li>We have fine-tuned a deep convolutional neural network (CNN) based on the <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/">VGG-face</a> architecture, in order to authenticate one of our team members (Laurynas) as an administrator of a secure system;</li>
<li>The authentication system leverages a laptop web cam—detected faces in the camera’s view are submitted to the network for classification;</li>
<li>We have planted a “hack switch”, capable of intercepting the input and performing adversarial training on it before submitting it for classification—this resulted in a 100% success rate for authenticating as Laurynas, regardless of your facial features.</li>
</ul>
<p>Hopefully, FaceJack has achieved its objective of highlighting this important issue in a clear and concise fashion. We hope to expand it in the near future with further authentication attacks, for example speech recognition-based ones.</p>

<
	</div>
</div><!-- /.container -->


</body>
</html>
